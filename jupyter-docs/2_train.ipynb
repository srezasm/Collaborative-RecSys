{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from settings import *\n",
    "from utils import load_np_arr, save_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "dev = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {dev} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|General <br />  Notation  | Description                                                                | Python (if any) |\n",
    "|:-------------------------|:---------------------------------------------------------------------------|-----------------|\n",
    "| $r(i,j)$                 | scalar; = 1  if user j rated movie i  = 0  otherwise                       |                 |\n",
    "| $y(i,j)$                 | scalar; = rating given by user j on movie  i    (if r(i,j) = 1 is defined) |                 |\n",
    "| $\\mathbf{w}^{(j)}$       | vector; parameters for user j                                              |                 |\n",
    "| $b^{(j)}$                | scalar; parameter for user j                                               |                 |\n",
    "| $\\mathbf{x}^{(i)}$       | vector; feature ratings for movie i                                        |                 |     \n",
    "| $n_m$                    | number of movies                                                           | num_movies      |\n",
    "| $n_u$                    | number of users                                                            | num_users       |\n",
    "| $n$                      | number of features                                                         | num_features    |\n",
    "| $\\mathbf{X}$             | matrix of vectors $\\mathbf{x}^{(i)}$                                       | X               |\n",
    "| $\\mathbf{W}$             | matrix of vectors $\\mathbf{w}^{(j)}$                                       | W               |\n",
    "| $\\mathbf{b}$             | vector of bias parameters $b^{(j)}$                                        | b               |\n",
    "| $\\mathbf{R}$             | matrix of elements $r(i,j)$                                                | R               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collaborative filtering cost function is given by\n",
    "\n",
    "$$\n",
    "J({\\mathbf{x}^{(0)},\\dots,\\mathbf{x}^{(n_m-1)},\\mathbf{w}^{(0)},b^{(0)},\\dots,\\mathbf{w}^{(n_u-1)},b^{(n_u-1)}}) = \\left[\\frac{1}{2}\\sum_{(i,j):r(i,j)=1} (\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right] + \\underbrace{\\left[\\frac{\\lambda}{2} \\sum^{n_u-1}_{j=0} \\sum^{n-1}_{k=0} (\\mathbf{w}^{(j)}_k)^2 + \\frac{\\lambda}{2} \\sum^{n_m-1}_{i=0} \\sum^{n-1}_{k=0} (\\mathbf{x}^{(i)}_k)^2 \\right]}_{\\text{regularization}}\n",
    "$$\n",
    "\n",
    "The first summation in (1) is \"for all $i$, $j$ where $r(i,j)$ equals $1$\" and could be written:\n",
    "\n",
    "$$\n",
    "= \\left[ \\frac{1}{2}\\sum_{j=0}^{n_u-1} \\sum_{i=0}^{n_m-1}r(i,j)*(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right] + \\text{regularization}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func(X: torch.Tensor, W: torch.Tensor, b: torch.Tensor,\n",
    "                   Y: torch.Tensor, R: torch.Tensor, lambda_: torch.float) -> torch.float:\n",
    "    z = torch.matmul(X, W.T)\n",
    "    z = z.add(b)\n",
    "    j = z.subtract(Y)\n",
    "    j = j.pow(2)\n",
    "    j = j.multiply(R)\n",
    "    j = j.sum()\n",
    "    j = j.multiply(0.5)\n",
    "\n",
    "    reg_val = (lambda_ / 2) * (W.pow(2).sum() + X.pow(2).sum())\n",
    "\n",
    "    return j + reg_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and convert the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = load_np_arr(Y_FILE_NAME)\n",
    "R = load_np_arr(R_FILE_NAME)\n",
    "\n",
    "Y = torch.tensor(Y, device=dev)\n",
    "R = torch.tensor(R, device=dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. features: 100\n",
      "Num. movies:   9724\n",
      "Num. users:    610\n"
     ]
    }
   ],
   "source": [
    "num_features = 100\n",
    "num_movies = R.shape[0]\n",
    "num_users = R.shape[1]\n",
    "\n",
    "print('Num. features:', num_features)\n",
    "print('Num. movies:  ', num_movies)\n",
    "print('Num. users:   ', num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 40620.94267767584\n"
     ]
    }
   ],
   "source": [
    "X = nn.Parameter(\n",
    "    torch.Tensor(num_movies, num_features).to(dev)\n",
    ")\n",
    "W = nn.Parameter(\n",
    "    torch.Tensor(num_users, num_features).to(dev)\n",
    ")\n",
    "b = nn.Parameter(\n",
    "    torch.Tensor(1, num_users).to(dev)\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize parameters\n",
    "nn.init.kaiming_uniform_(X, a=math.sqrt(5)).to(dev)\n",
    "nn.init.kaiming_uniform_(W, a=math.sqrt(5)).to(dev)\n",
    "fan_in, _ = nn.init._calculate_fan_in_and_fan_out(W)\n",
    "bound = 1 / math.sqrt(fan_in)\n",
    "nn.init.uniform_(b, -bound, bound).to(dev)\n",
    "\n",
    "print(f'Initial loss: {cofi_cost_func(X, W, b, Y, R, lambda_=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=[X, W, b], lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       Loss\n",
      "    1       40620.94268\n",
      "  100       39073.83712\n",
      "  200       37416.08541\n",
      "  300       35435.24266\n",
      "  400       33095.44997\n",
      "  500       30518.86562\n",
      "  600       27901.94262\n",
      "  700       25397.83804\n",
      "  800       23077.78437\n",
      "  900       20962.85165\n",
      " 1000       19055.26132\n",
      " 1100       17350.08761\n",
      " 1200       15837.86457\n",
      " 1300       14505.39909\n",
      " 1400       13336.90183\n",
      " 1500       12315.35440\n",
      " 1600       11423.73117\n",
      " 1700       10645.86042\n",
      " 1800       9966.91210\n",
      " 1900       9373.60209\n",
      " 2000       8854.21604\n",
      " 2100       8398.53835\n",
      " 2200       7997.73287\n",
      " 2300       7644.19621\n",
      " 2400       7331.41784\n",
      " 2500       7053.83903\n",
      " 2600       6806.72237\n",
      " 2700       6586.03499\n",
      " 2800       6388.34182\n",
      " 2900       6210.71515\n",
      " 3000       6050.65477\n",
      " 3100       5906.02248\n",
      " 3200       5774.98569\n",
      " 3300       5655.96705\n",
      " 3400       5547.60655\n",
      " 3500       5448.72386\n",
      " 3600       5358.29117\n",
      " 3700       5275.41018\n",
      " 3800       5199.29001\n",
      " 3900       5129.23237\n",
      " 4000       5064.61955\n",
      " 4100       5004.90329\n",
      " 4200       4949.59638\n",
      " 4300       4898.26592\n",
      " 4400       4850.52747\n",
      " 4500       4806.03707\n",
      " 4600       4764.49149\n",
      " 4700       4725.62150\n",
      " 4800       4689.18643\n",
      " 4900       4654.97366\n",
      " 5000       4622.79253\n",
      " 5100       4592.47508\n",
      " 5200       4563.87044\n",
      " 5300       4536.84376\n",
      " 5400       4511.27359\n",
      " 5500       4487.05072\n",
      " 5600       4464.07728\n",
      " 5700       4442.26341\n",
      " 5800       4421.52856\n",
      " 5900       4401.79916\n",
      " 6000       4383.00719\n",
      " 6100       4365.09145\n",
      " 6200       4347.99542\n",
      " 6300       4331.66721\n",
      " 6400       4316.05897\n",
      " 6500       4301.12669\n",
      " 6600       4286.82999\n",
      " 6700       4273.13112\n",
      " 6800       4259.99533\n",
      " 6900       4247.39048\n",
      " 7000       4235.28674\n",
      " 7100       4223.65647\n",
      " 7200       4212.47368\n",
      " 7300       4201.71479\n",
      " 7400       4191.35812\n",
      " 7500       4181.38267\n",
      " 7600       4171.76969\n",
      " 7700       4162.50210\n",
      " 7800       4153.56355\n",
      " 7900       4144.93890\n",
      " 8000       4136.61481\n",
      " 8100       4128.57766\n",
      " 8200       4120.81597\n",
      " 8300       4113.31890\n",
      " 8400       4106.07581\n",
      " 8500       4099.07706\n",
      " 8600       4092.31368\n",
      " 8700       4085.77724\n",
      " 8800       4079.45966\n",
      " 8900       4073.35297\n",
      " 9000       4067.45002\n",
      " 9100       4061.74422\n",
      " 9200       4056.22898\n",
      " 9300       4050.89735\n",
      " 9400       4045.74420\n",
      " 9500       4040.76320\n",
      " 9600       4035.94945\n",
      " 9700       4031.29772\n",
      " 9800       4026.80290\n",
      " 9900       4022.46008\n",
      "10000       4018.26499\n",
      "10100       4014.21355\n",
      "10200       4010.30142\n",
      "10300       4006.52479\n",
      "10400       4002.87951\n",
      "10500       3999.36218\n",
      "10600       3995.96893\n",
      "10700       3992.69642\n",
      "10800       3989.54128\n",
      "10900       3986.49959\n",
      "11000       3983.56917\n",
      "11100       3980.74560\n",
      "11200       3978.02714\n",
      "11300       3975.40999\n",
      "11400       3972.89129\n",
      "11500       3970.46949\n",
      "11600       3968.14033\n",
      "11700       3965.90263\n",
      "11800       3963.75298\n",
      "11900       3961.68967\n",
      "12000       3959.70945\n",
      "12100       3957.80985\n",
      "12200       3955.98851\n",
      "12300       3954.24293\n",
      "12400       3952.56986\n",
      "12500       3950.96633\n",
      "12600       3949.42913\n",
      "12700       3947.95571\n",
      "12800       3946.54230\n",
      "12900       3945.18627\n",
      "13000       3943.88429\n",
      "13100       3942.63392\n",
      "13200       3941.43270\n",
      "13300       3940.27765\n",
      "13400       3939.16740\n",
      "13500       3938.10068\n",
      "13600       3937.07573\n",
      "13700       3936.09218\n",
      "13800       3935.14886\n",
      "13900       3934.24535\n",
      "14000       3933.38208\n",
      "14100       3932.55769\n",
      "14200       3931.77173\n",
      "14300       3931.02384\n",
      "14400       3930.31349\n",
      "14500       3929.63831\n",
      "14600       3928.99836\n",
      "14700       3928.39044\n",
      "14800       3927.81306\n",
      "14900       3927.26462\n",
      "15000       3926.74268\n"
     ]
    }
   ],
   "source": [
    "epochs = 15001\n",
    "lambda_ = 1\n",
    "\n",
    "print('Epoch       Loss')\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    cost_val = cofi_cost_func(X, W, b, Y, R, lambda_)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost_val.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'{epoch:5}       {cost_val:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save_tensor(W, W_FILE_NAME))\n",
    "print(save_tensor(X, X_FILE_NAME))\n",
    "print(save_tensor(b, B_FILE_NAME))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
