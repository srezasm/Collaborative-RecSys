{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from settings import *\n",
    "from utils import load_np_arr, save_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "dev = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {dev} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and convert the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = load_np_arr(Y_FILE_NAME)\n",
    "R = load_np_arr(R_FILE_NAME)\n",
    "mu = load_np_arr(MU_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. features: 100\n",
      "Num. movies:   9724\n",
      "Num. users:    610\n"
     ]
    }
   ],
   "source": [
    "num_features = 100\n",
    "num_movies = R.shape[0]\n",
    "num_users = R.shape[1]\n",
    "\n",
    "print('Num. features:', num_features)\n",
    "print('Num. movies:  ', num_movies)\n",
    "print('Num. users:   ', num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually rate some movies by their id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ratings = np.zeros(num_movies)\n",
    "\n",
    "my_ratings[314] = 5 # Forrest Gump\n",
    "my_ratings[461] = 5 # Schindler's List\n",
    "\n",
    "my_ratings[904] = 5 # 12 Angry Men\n",
    "my_ratings[7315] = 5 # 12 Angry Men too! we rate them both just in case\n",
    "\n",
    "my_ratings[3635] = 5 # A Beautiful Mind (2001)\n",
    "my_ratings[8240] = 5 # Rush (2013)\n",
    "my_ratings[8448] = 5 # Whiplash (2014)\n",
    "my_ratings[9363] = 4 # Your Name. (2016)\n",
    "\n",
    "my_ratings[7675] = 0.5 # The Avengers (2012)\n",
    "my_ratings[8668] = 0.5 # Avengers: Age of Ultron\n",
    "my_ratings[8675] = 0.5 # Avengers: Infinity War - Part I\n",
    "my_ratings[1985] = 0.5 # Superman\n",
    "my_ratings[1986] = 0.5 # Superman II\n",
    "my_ratings[1987] = 0.5 # Superman III\n",
    "my_ratings[1988] = 0.5 # Superman IV: The Quest for Peace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append `my_ratings` to the `R` amd `Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the normalized my_ratings to Y\n",
    "Y = np.c_[Y, my_ratings - mu.squeeze()]\n",
    "\n",
    "# Append the binary my_ratings to R\n",
    "R = np.c_[R, (my_ratings != 0).astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increment `num_users` after appending new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|General <br />  Notation  | Description                                                                | Python (if any) |\n",
    "|:-------------------------|:---------------------------------------------------------------------------|-----------------|\n",
    "| $r(i,j)$                 | scalar; = 1  if user j rated movie i  = 0  otherwise                       |                 |\n",
    "| $y(i,j)$                 | scalar; = rating given by user j on movie  i    (if r(i,j) = 1 is defined) |                 |\n",
    "| $\\mathbf{w}^{(j)}$       | vector; parameters for user j                                              |                 |\n",
    "| $b^{(j)}$                | scalar; parameter for user j                                               |                 |\n",
    "| $\\mathbf{x}^{(i)}$       | vector; feature ratings for movie i                                        |                 |     \n",
    "| $n_m$                    | number of movies                                                           | num_movies      |\n",
    "| $n_u$                    | number of users                                                            | num_users       |\n",
    "| $n$                      | number of features                                                         | num_features    |\n",
    "| $\\mathbf{X}$             | matrix of vectors $\\mathbf{x}^{(i)}$                                       | X               |\n",
    "| $\\mathbf{W}$             | matrix of vectors $\\mathbf{w}^{(j)}$                                       | W               |\n",
    "| $\\mathbf{b}$             | vector of bias parameters $b^{(j)}$                                        | b               |\n",
    "| $\\mathbf{R}$             | matrix of elements $r(i,j)$                                                | R               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collaborative filtering cost function is given by\n",
    "\n",
    "$$\n",
    "J({\\mathbf{x}^{(0)},\\dots,\\mathbf{x}^{(n_m-1)},\\mathbf{w}^{(0)},b^{(0)},\\dots,\\mathbf{w}^{(n_u-1)},b^{(n_u-1)}}) = \\left[\\frac{1}{2}\\sum_{(i,j):r(i,j)=1} (\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right] + \\underbrace{\\left[\\frac{\\lambda}{2} \\sum^{n_u-1}_{j=0} \\sum^{n-1}_{k=0} (\\mathbf{w}^{(j)}_k)^2 + \\frac{\\lambda}{2} \\sum^{n_m-1}_{i=0} \\sum^{n-1}_{k=0} (\\mathbf{x}^{(i)}_k)^2 \\right]}_{\\text{regularization}}\n",
    "$$\n",
    "\n",
    "The first summation in (1) is \"for all $i$, $j$ where $r(i,j)$ equals $1$\" and could be written:\n",
    "\n",
    "$$\n",
    "= \\left[ \\frac{1}{2}\\sum_{j=0}^{n_u-1} \\sum_{i=0}^{n_m-1}r(i,j)*(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right] + \\text{regularization}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func(X: torch.Tensor, W: torch.Tensor, b: torch.Tensor,\n",
    "                   Y: torch.Tensor, R: torch.Tensor, lambda_: torch.float) -> torch.float:\n",
    "    z = torch.matmul(X, W.T)\n",
    "    z = z.add(b)\n",
    "    j = z.subtract(Y)\n",
    "    j = j.pow(2)\n",
    "    j = j.multiply(R)\n",
    "    j = j.sum()\n",
    "    j = j.multiply(0.5)\n",
    "\n",
    "    reg_val = (lambda_ / 2) * (W.pow(2).sum() + X.pow(2).sum())\n",
    "\n",
    "    return j + reg_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `R` and `Y` to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor(Y, device=dev)\n",
    "R = torch.tensor(R, device=dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 40497.382090237494\n"
     ]
    }
   ],
   "source": [
    "X = nn.Parameter(\n",
    "    torch.Tensor(num_movies, num_features).to(dev)\n",
    ")\n",
    "W = nn.Parameter(\n",
    "    torch.Tensor(num_users, num_features).to(dev)\n",
    ")\n",
    "b = nn.Parameter(\n",
    "    torch.Tensor(1, num_users).to(dev)\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize parameters\n",
    "nn.init.kaiming_uniform_(X, a=math.sqrt(5)).to(dev)\n",
    "nn.init.kaiming_uniform_(W, a=math.sqrt(5)).to(dev)\n",
    "fan_in, _ = nn.init._calculate_fan_in_and_fan_out(W)\n",
    "bound = 1 / math.sqrt(fan_in)\n",
    "nn.init.uniform_(b, -bound, bound).to(dev)\n",
    "\n",
    "print(f'Initial loss: {cofi_cost_func(X, W, b, Y, R, lambda_=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=[X, W, b], lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch       Loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100       38971.61455\n",
      "  200       37316.92164\n",
      "  300       35337.73749\n",
      "  400       33002.74322\n",
      "  500       30440.43454\n",
      "  600       27849.04441\n",
      "  700       25375.66029\n",
      "  800       23083.31637\n",
      "  900       20988.91764\n",
      " 1000       19094.52744\n",
      " 1100       17396.71030\n",
      " 1200       15887.60845\n",
      " 1300       14555.35046\n",
      " 1400       13385.12150\n",
      " 1500       12360.61044\n",
      " 1600       11465.30948\n",
      " 1700       10683.42040\n",
      " 1800       10000.37468\n",
      " 1900       9403.07429\n",
      " 2000       8879.94450\n",
      " 2100       8420.88098\n",
      " 2200       8017.13122\n",
      " 2300       7661.15175\n",
      " 2400       7346.46179\n",
      " 2500       7067.49636\n",
      " 2600       6819.48263\n",
      " 2700       6598.32522\n",
      " 2800       6400.51126\n",
      " 2900       6223.02951\n",
      " 3000       6063.30095\n",
      " 3100       5919.11495\n",
      " 3200       5788.57869\n",
      " 3300       5670.06741\n",
      " 3400       5562.18238\n",
      " 3500       5463.71822\n",
      " 3600       5373.63161\n",
      " 3700       5291.01414\n",
      " 3800       5215.07277\n",
      " 3900       5145.11272\n",
      " 4000       5080.52288\n",
      " 4100       5020.76222\n",
      " 4200       4965.35335\n",
      " 4300       4913.87332\n",
      " 4400       4865.94553\n",
      " 4500       4821.23660\n",
      " 4600       4779.45099\n",
      " 4700       4740.32415\n",
      " 4800       4703.62218\n",
      " 4900       4669.13631\n",
      " 5000       4636.67940\n",
      " 5100       4606.08462\n",
      " 5200       4577.20363\n",
      " 5300       4549.90139\n",
      " 5400       4524.05851\n",
      " 5500       4499.56523\n",
      " 5600       4476.32491\n",
      " 5700       4454.24813\n",
      " 5800       4433.25572\n",
      " 5900       4413.27333\n",
      " 6000       4394.23539\n",
      " 6100       4376.08122\n",
      " 6200       4358.75549\n",
      " 6300       4342.20718\n",
      " 6400       4326.38871\n",
      " 6500       4311.25706\n",
      " 6600       4296.77242\n",
      " 6700       4282.89727\n",
      " 6800       4269.59689\n",
      " 6900       4256.83981\n",
      " 7000       4244.59535\n",
      " 7100       4232.83627\n",
      " 7200       4221.53663\n",
      " 7300       4210.67160\n",
      " 7400       4200.21935\n",
      " 7500       4190.15858\n",
      " 7600       4180.46998\n",
      " 7700       4171.13471\n",
      " 7800       4162.13590\n",
      " 7900       4153.45803\n",
      " 8000       4145.08602\n",
      " 8100       4137.00665\n",
      " 8200       4129.20647\n",
      " 8300       4121.67409\n",
      " 8400       4114.39808\n",
      " 8500       4107.36859\n",
      " 8600       4100.57560\n",
      " 8700       4094.01018\n",
      " 8800       4087.66425\n",
      " 8900       4081.52902\n",
      " 9000       4075.59812\n",
      " 9100       4069.86341\n",
      " 9200       4064.31854\n",
      " 9300       4058.95667\n",
      " 9400       4053.77240\n",
      " 9500       4048.75897\n",
      " 9600       4043.91152\n",
      " 9700       4039.22395\n",
      " 9800       4034.69171\n",
      " 9900       4030.30951\n",
      "10000       4026.07367\n",
      "10100       4021.97882\n",
      "10200       4018.02143\n",
      "10300       4014.19717\n",
      "10400       4010.50232\n",
      "10500       4006.93392\n",
      "10600       4003.48759\n",
      "10700       4000.16104\n",
      "10800       3996.95098\n",
      "10900       3993.85401\n",
      "11000       3990.86777\n",
      "11100       3987.98857\n",
      "11200       3985.21390\n",
      "11300       3982.54172\n",
      "11400       3979.96832\n",
      "11500       3977.49092\n",
      "11600       3975.10746\n",
      "11700       3972.81465\n",
      "11800       3970.60942\n",
      "11900       3968.48898\n",
      "12000       3966.45095\n",
      "12100       3964.49217\n",
      "12200       3962.61015\n",
      "12300       3960.80153\n",
      "12400       3959.06426\n",
      "12500       3957.39564\n",
      "12600       3955.79321\n",
      "12700       3954.25447\n",
      "12800       3952.77788\n",
      "12900       3951.36091\n",
      "13000       3950.00158\n",
      "13100       3948.69797\n",
      "13200       3947.44855\n",
      "13300       3946.25063\n",
      "13400       3945.10191\n",
      "13500       3944.00129\n",
      "13600       3942.94514\n",
      "13700       3941.93261\n",
      "13800       3940.96083\n",
      "13900       3940.02795\n",
      "14000       3939.13211\n",
      "14100       3938.27259\n",
      "14200       3937.44697\n",
      "14300       3936.65482\n",
      "14400       3935.89459\n",
      "14500       3935.16528\n",
      "14600       3934.46549\n",
      "14700       3933.79415\n",
      "14800       3933.15000\n",
      "14900       3932.53114\n",
      "15000       3931.93705\n"
     ]
    }
   ],
   "source": [
    "epochs = 15001\n",
    "lambda_ = 1\n",
    "\n",
    "print('Epoch       Loss')\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    cost_val = cofi_cost_func(X, W, b, Y, R, lambda_)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost_val.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'{epoch:5}       {cost_val:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tensor in ./cache/W.pt\n",
      "Saved tensor in ./cache/X.pt\n",
      "Saved tensor in ./cache/b.pt\n"
     ]
    }
   ],
   "source": [
    "print(save_tensor(W, W_FILE_NAME))\n",
    "print(save_tensor(X, X_FILE_NAME))\n",
    "print(save_tensor(b, B_FILE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
